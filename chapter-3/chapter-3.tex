\chapter{Distribution And Fusion Case Study}
\label{chap:ch3}

\section{Background}
    The optimization in the compiler is very important of code generation.
Good the optimization, better is the code execution. Here, we will talk about Loop Distribution optimization.

    In Loop Distribution, the bigger loop is split into smaller ones. It may lead to improved loop vectorization and better locality. In LLVM the loop distribution is not efficient is heuristic-based and does not promise the optimal distribution. We are presenting the ML-based Loop Distribution.


\section{Motivation}
To achieve the optimal distribution, we first want to have max distribution. We have an ideology that max fusion can lead to max distribution. We have to perform fusion in the pipeline to have better distribution and indeed may lead to better vectorization. To see that whether there are fusion opportunities present in the flow or not. We have also written an inline enabling pass for the functions in the innermost loop. We run experimentation on the SPEC 2017 benchmarks to observe the fusion candidate.



\begin{table}[h]
\begin{tabular}{|l|l|l|}
\hline
\multirow{2}{*}{\textbf{BENCHMARK}} & \multicolumn{2}{c|}{\textbf{\#Candidates for Loop Fusion}} \\ \cline{2-3} 
 & \textbf{O3} & \textbf{O3-inline-Enabled} \\ \hline
blender\_r & 3952 & 7042 \\ \hline
deepsjeng\_r & 94 & 244 \\ \hline
imagick\_r & 2735 & 3065 \\ \hline
lbm\_r & 8 & 8 \\ \hline
leela\_r & 53 & 69 \\ \hline
mcf\_r & 14 & 16 \\ \hline
namd\_r & 1392 & 1812 \\ \hline
omnetpp\_r & 203 & 2200 \\ \hline
povray\_r & 229 & 2232 \\ \hline
x264\_r & 356 & 760 \\ \hline
xalancbmk\_r & 1017 & 1147 \\ \hline
xz\_r & 67 & 85 \\ \hline
\end{tabular}
\centering
\end{table}


\section{Model}
 The generated graphs are used as inputs in our modeling pipeline. In this section, we explain how we represent SDGs using IR2Vec representations and the way we model the reinforcement learning model to consume this information using Gated Graph Neural Networks. We also discuss various parameters of the model, including the design of our cost model, with which we arrive at a relative number of clock ticks that take into account impact by both locality and vectorization.

% CONSTRAINTS ON SDG
% TOPOLOGICAL MATH FORMULATION
% MERGE AND DISTRIBUTE

\subsection{GGNN based reinforcement learning model}
    Fig. 2 shows the workflow of the reinforcement learning model for our problem. The model takes SDG as input and maps it to a GGNN. The IR2Vec embeddings take the type of operation, a variable is involved into consideration.

    GGNN[Y Li] is the upgradation of the graph neural network by adding an Gating mechanism using the Gated Recurrent Units  and unroll the recurrence
for a fixed number of steps T. Each node of the GGNN has a hidden presentation which is initially represented using IR2Vec embeddings and dannotation vectors. 

In our work, we use the propagation model of the GGNN to get the updated hidden state vectors at any instance of time, 

Initial Presentation 
hno = IR2vec Embedding according to SDG node   
Hn0 : [hno:Ano]  
A is the meta vector [isStartNode, IsVisited]
Ano  = [0,0]  as no node visited in the starting

At any instant t for node n
Hnt-1 : [hnt-1:Ant-1]
f : Hnt-1 → hant-1 
% x :  ∑n’ e neigh(n) han’t-1 + b
hnt	 : GRU(hant-1,, x)

Using this mathematical calculation, we are updating the hidden nodes.
 
    The set of all possible visitable nodes at the start of the topological walk is the initial state of the environment. The mode takes the merge decision from the set of legal node choices given by topological walk on GGNN. The agent makes a decision to which node to visit and after visiting, what subsequent action should be taken(merge, distribute). For new node visited we updated the annotation isVisited to True. If subsequent decision is merge, then we update the GGNN with a new edge called ‘Merge Edge’ and is placed  between the source and visited node. If subsequent decision is distribute, we do not apply any change to the GGNN structure. Then we perform propagation on the GGNN to update the hidden state of the nodes. We continue this till all the nodes are visited.

\subsection{RL Model}
Parameters of RL Model → State, action, reward
We are trying to represent our problem as the Markov decision Problem. We try to solve this problem using the reinforcement learning setting. We convert the SDG graph to GGNN and get the state of the environment at any instance of time. We are trying to solve this problem in partial observable space. 

\paragraph{State} It is the presentation of the environment at an instant of time. We are using GGNN to capture the state of the environment.
Si = ({Set of all the possible visitable Nodes from the current node}) 
Here we have added a constraint that nodes are selected as per the topological walk.   

\paragraph{Initial state} At the start point, the state will be all the set of all the possible visitable Nodes..
Si = ({Set of eligible Node}, focusNode) focusNode=None 

\paragraph{Action} 
Given a state, select the next node in the graph to be visited by the graph followed by the merge or distribute decision. This is constrained by the topological sort to maintain the legality of the program.
If no node visited till now:
	A = (select nextnode from state)
else
% A = (select nextnode from state, D \ɛ {Merge distributed})
After action is taken, we update the environment and get the next state.

\paragraph{Reward} For every action, the environment gives back a feedback which is called reward. It signifies the goodness of the action. In our work, we are using a static cost model sec 3.6 to calculate the reward by using the formulae LCo-LCd/LCo(where LC is the LoopCost). We use reward based on the loopCost at the end of the topological walk and for the intermediate action, we give zero as reward. The negative reward signifies that the distributed loop is not efficient. In RL, we want to maximize the rewards.

	Use DQN Algorithm to perform training the 1980 loops. The batch size of 64 with replay buffer size of 10000. To introduce  exploration in the training, we gradually decrease the epsilon with time. 
